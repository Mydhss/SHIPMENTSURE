# -*- coding: utf-8 -*-
"""ShipmentSure_Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l4K6cVsnnMpuvQGPB36l8Hh2pDJGfREk
"""

# Install necessary libraries
!pip install seaborn matplotlib pandas

# Import libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

import pandas as pd

data = pd.read_csv("Train.csv")
data



print(" Dataset after cleaning:\n")
print(data.info())
print("\nMissing values per column:\n", data.isnull().sum())
data.head()

# Check and remove duplicates
print("Number of duplicate rows before:", data.duplicated().sum())
data.drop_duplicates(inplace=True)
print(" Number of duplicate rows after:", data.duplicated().sum())

# Handle outliers using IQR capping
import numpy as np
num_cols = data.select_dtypes(include=["int64", "float64"]).columns.tolist()


for col in data.select_dtypes(include=['float64', 'int64']).columns:
    Q1 = data[col].quantile(0.25)
    Q3 = data[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    data[col] = np.where(data[col] < lower, lower,
                       np.where(data[col] > upper, upper, data[col]))
print(" Outliers capped using IQR method")

# Standardize categorical values
cat_cols = data.select_dtypes(include=["object"]).columns.tolist()
for col in cat_cols:
    data[col] = data[col].str.strip().str.lower()

print(" Standardized categorical column values")
print({col: data[col].unique().tolist() for col in cat_cols})

num_cols = data.select_dtypes(include=["int64", "float64"]).columns.tolist()
num_cols.remove("Reached.on.Time_Y.N")  # remove target from analysis

sns.set(style="whitegrid", palette="Set2")

# Histograms
for col in num_cols:
    plt.figure(figsize=(6,4))
    sns.histplot(data[col], kde=True, bins=30)
    plt.title(f"Distribution of {col}")
    plt.show()



cat_cols = data.select_dtypes(include=["object"]).columns.tolist()

# Countplots
for col in cat_cols:
    plt.figure(figsize=(6,4))
    sns.countplot(data=data, x=col, order=data[col].value_counts().index)
    plt.title(f"Countplot of {col}")
    plt.xticks(rotation=45)
    plt.show()



target = "Reached.on.Time_Y.N"

# Boxplots
for col in num_cols:
    plt.figure(figsize=(6,4))
    sns.boxplot(x=target, y=col, data=data)
    plt.title(f"{col} vs {target}")
    plt.show()

# KDE plots split by target
for col in num_cols:
    plt.figure(figsize=(6,4))
    sns.kdeplot(data=data, x=col, hue=target, fill=True)
    plt.title(f"Distribution of {col} by {target}")
    plt.show()



# Countplots with hue = target
for col in cat_cols:
    plt.figure(figsize=(6,4))
    sns.countplot(data=data, x=col, hue=target)
    plt.title(f"{col} vs {target}")
    plt.xticks(rotation=45)
    plt.show()



plt.figure(figsize=(8,6))
sns.heatmap(data[num_cols+[target]].corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Heatmap")
plt.show()



# Examine target variable balance
target = "Reached.on.Time_Y.N"

# Basic count
print(" Class distribution:")
print(data[target].value_counts())
print("\n Class distribution (in %):")
print(round(data[target].value_counts(normalize=True) * 100, 2))

# Visualize the imbalance
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(5,4))
sns.countplot(x=target, data=data, palette="Set2")
plt.title("Class Distribution of Target Variable")
plt.xlabel("Reached on Time (0 = On-time, 1 = Delayed)")
plt.ylabel("Count")
plt.show()

r = pd.crosstab(data['Warehouse_block'],data['Reached.on.Time_Y.N'])
r.plot(kind='bar')
plt.show()

# Copy dataset
encode_data = data.copy()

# Initialize LabelEncoder
from sklearn.preprocessing import LabelEncoder # Import LabelEncoder
le = LabelEncoder()

# Label Encode
encode_data['Product_importance'] = le.fit_transform(encode_data['Product_importance'])
encode_data['Gender'] = le.fit_transform(encode_data['Gender'])

# One-Hot Encode nominal categorical columns
encode_data = pd.get_dummies(encode_data,
                             columns=['Warehouse_block', 'Mode_of_Shipment'],
                             drop_first=True)

# Check
encode_data.head()

encode_data[['Product_importance', 'Gender'] +
            [col for col in encode_data.columns if 'Warehouse_block' in col or 'Mode_of_Shipment' in col]].head()
# Convert all boolean columns to 0/1
encode_data = encode_data.astype(int)
encode_data.head()

from sklearn.preprocessing import StandardScaler

std_scaler = StandardScaler()
# Columns to normalize
num_cols = ['Customer_care_calls', 'Customer_rating', 'Cost_of_the_Product',
            'Prior_purchases', 'Discount_offered', 'Weight_in_gms']

# Apply to numerical columns
std_scaled_data = encode_data.copy()
std_scaled_data[num_cols] = std_scaler.fit_transform(encode_data[num_cols])

# Check results
print(std_scaled_data[num_cols].describe())

encode_data['Cost_to_Weight_ratio'] = encode_data['Cost_of_the_Product'] / encode_data['Weight_in_gms']

#Handling infinity and NaN values
encode_data.replace([np.inf, -np.inf], np.nan, inplace=True)

# Fill NaN values  with the median of valid ratios

print("Any NaN values left?", encode_data.isnull().sum().sum())
print("\nCost_to_Weight_ratio summary:\n", encode_data['Cost_to_Weight_ratio'].describe())

# Preview final dataset
encode_data.head()

print("Target Variable (Reached.on.Time_Y.N):")
print(data['Reached.on.Time_Y.N'].value_counts())
print("\nNormalized Value Counts (Proportion):")
print(data['Reached.on.Time_Y.N'].value_counts(normalize=True))

categorical_cols = ['Warehouse_block', 'Mode_of_Shipment', 'Product_importance', 'Gender']

for col in categorical_cols:
    print(f"\nColumn: {col}")
    print(data[col].value_counts())
    print("\nNormalized:")
    print(data[col].value_counts(normalize=True))

encode_data.select_dtypes(include=['object']).columns
encode_data.dtypes

X = encode_data.drop('Reached.on.Time_Y.N', axis=1)
y = encode_data['Reached.on.Time_Y.N']

X

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
# Further split training data into train + validation
X_train_sub, X_val, y_train_sub, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)

print("Training set shape:", X_train.shape)
print("Testing set shape:", X_test.shape)

from imblearn.over_sampling import SMOTE
from collections import Counter

smote = SMOTE(random_state=42)
# Apply to training data
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)
# Check new class distribution
print("After SMOTE class distribution:", Counter(y_train_resampled))

!pip install catboost

from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.svm import SVC  # Support Vector Machine
from xgboost import XGBClassifier
from sklearn.naive_bayes import GaussianNB # Import GaussianNB
from sklearn.neighbors import KNeighborsClassifier # Import KNeighborsClassifier
from lightgbm import LGBMClassifier # Import LightGBM
from catboost import CatBoostClassifier # Import CatBoost


cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
models = {
    "Logistic Regression": LogisticRegression(C=0.5, max_iter=1000, random_state=42),  # L2 regularization (smaller C = stronger)

    "Decision Tree": DecisionTreeClassifier(max_depth=8, min_samples_split=10, min_samples_leaf=5, random_state=42),

    "Random Forest": RandomForestClassifier(n_estimators=200, max_depth=10, min_samples_split=10, min_samples_leaf=5, random_state=42, n_jobs=-1),

    "Naive Bayes": GaussianNB(),

    "KNN": KNeighborsClassifier(n_neighbors=9),  # more neighbors ‚Üí smoother, less overfitting

    "SVM": SVC(C=0.5, kernel='rbf', gamma='scale', random_state=42),

    "XGBoost": XGBClassifier(
        learning_rate=0.05, n_estimators=500, max_depth=6,subsample=0.8, colsample_bytree=0.8, reg_lambda=0.5, use_label_encoder=False, eval_metric='logloss', random_state=42),

    "LightGBM": LGBMClassifier(
        learning_rate=0.05, n_estimators=500, max_depth=6,num_leaves=20, subsample=0.8, colsample_bytree=0.8, reg_alpha=0.2, reg_lambda=0.5, random_state=42),

    "CatBoost": CatBoostClassifier(
        iterations=500, learning_rate=0.05, depth=6, l2_leaf_reg=5, verbose=False, random_state=42)
}

from sklearn.pipeline import Pipeline
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler

results = []

for name, model in models.items():
    pipe = Pipeline([('scaler', StandardScaler()), ('model', model)])

    # Cross-validation accuracy (10-fold)
    cv_scores = cross_val_score(pipe, X_train, y_train, cv=cv, scoring='accuracy', n_jobs=-1)

    results.append({
        'Model': name,
        'Mean CV Accuracy': round(np.mean(cv_scores), 4),
        'Std Dev': round(np.std(cv_scores), 4)
    })

results_df = pd.DataFrame(results).sort_values(by='Mean CV Accuracy', ascending=False)
print("\n Cross-validation results (Overfitting check):")
print(results_df)

RND = 42
# Define cross-validation strategy
cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=RND)
# Define models with anti-overfitting hyperparameters
models = {
    "Logistic Regression": LogisticRegression(C=0.1, penalty='l2', solver='liblinear', random_state=RND),
    "Decision Tree": DecisionTreeClassifier(max_depth=6, min_samples_split=10, min_samples_leaf=4, random_state=RND),
    "Random Forest": RandomForestClassifier(n_estimators=200, max_depth=10, min_samples_split=5,
                                            min_samples_leaf=3, max_features='sqrt', random_state=RND),
    "KNN": KNeighborsClassifier(n_neighbors=15),
    "Naive Bayes": GaussianNB(),
    "SVM": SVC(C=0.5, kernel='rbf', gamma='scale', random_state=RND),
    "XGBoost": XGBClassifier(max_depth=4, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8,
                             reg_lambda=1, n_estimators=200, random_state=RND, eval_metric='logloss', use_label_encoder=False),
    "LightGBM": LGBMClassifier(num_leaves=20, max_depth=6, subsample=0.8, reg_lambda=0.5,
                               learning_rate=0.05, n_estimators=200, random_state=RND),
    "CatBoost": CatBoostClassifier(depth=6, learning_rate=0.05, l2_leaf_reg=3, n_estimators=200,
                                   verbose=0, random_state=RND)
}

cv_results = []
for name, model in models.items():
    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')

    cv_results.append({
        "Model": name,
        "Mean CV Accuracy": np.mean(scores),
        "Std Dev": np.std(scores)
    })

cv_df = pd.DataFrame(cv_results).sort_values(by="Mean CV Accuracy", ascending=False)
print("\nCross-validation results (After Regularization to Fix Overfitting):")
print(cv_df.to_string(index=False))

from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.svm import SVC  # Support Vector Machine
from xgboost import XGBClassifier
from sklearn.naive_bayes import GaussianNB # Import GaussianNB
from sklearn.neighbors import KNeighborsClassifier # Import KNeighborsClassifier
from lightgbm import LGBMClassifier # Import LightGBM
from catboost import CatBoostClassifier # Import CatBoost


models = {
    "Logistic Regression": LogisticRegression(C=0.5, max_iter=1000, random_state=42),
    "Decision Tree": DecisionTreeClassifier(max_depth=8, min_samples_split=10, min_samples_leaf=5, random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=200, max_depth=10, min_samples_split=10, min_samples_leaf=5, random_state=42, n_jobs=-1),
    "Naive Bayes": GaussianNB(),
    "KNN": KNeighborsClassifier(n_neighbors=9),
    "SVM": SVC(C=0.5, kernel='rbf', gamma='scale', probability=True, random_state=42),
    "XGBoost": XGBClassifier(
        learning_rate=0.05, n_estimators=500, max_depth=6,
        subsample=0.8, colsample_bytree=0.8, reg_lambda=0.5,
        use_label_encoder=False, eval_metric='logloss', random_state=42),
    "LightGBM": LGBMClassifier(
        learning_rate=0.05, n_estimators=500, max_depth=6, num_leaves=20,
        subsample=0.8, colsample_bytree=0.8, reg_alpha=0.2, reg_lambda=0.5, random_state=42),
    "CatBoost": CatBoostClassifier(
        iterations=500, learning_rate=0.05, depth=6, l2_leaf_reg=5, verbose=False, random_state=42)
}

!pip install catboost

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, roc_curve, roc_auc_score, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns


comparison = []

for name, model in models.items():
    print(f"\nTraining and Evaluating: {name}")
    print("-" * 70)

    # Train model
    model.fit(X_train, y_train)
    y_pred = model.predict(X_val)
    y_prob = model.predict_proba(X_val)[:, 1]

    # Metrics
    acc = accuracy_score(y_val, y_pred)
    prec = precision_score(y_val, y_pred)
    rec = recall_score(y_val, y_pred)
    f1 = f1_score(y_val, y_pred)
    roc_auc = roc_auc_score(y_val, y_prob)


    print("\nClassification Report:")
    print(classification_report(y_val, y_pred))


    plt.figure(figsize=(5,4))
    sns.heatmap(pd.crosstab(y_val, y_pred), annot=True, fmt='d', cmap='Blues')
    plt.title(f"Confusion Matrix - {name}")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()



    fpr, tpr, _ = roc_curve(y_val, y_prob)
    plt.figure(figsize=(6,5))
    plt.plot(fpr, tpr, color='blue', lw=2, label=f"AUC = {roc_auc:.3f}")
    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
    plt.title(f"ROC Curve - {name}")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.legend(loc="lower right")
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.show()

    # Store Results
    comparison.append({
        "Model": name,
        "Accuracy": round(acc, 4),
        "Precision": round(prec, 4),
        "Recall": round(rec, 4),
        "F1 Score": round(f1, 4),
        "ROC-AUC": round(roc_auc, 4)
    })

comparison_df = pd.DataFrame(comparison).sort_values(by="ROC-AUC", ascending=False)
print("\nFinal Model Comparison Table:")
print(comparison_df)

import joblib
from sklearn.preprocessing import LabelEncoder

# ‚úÖ Step 1: Detect categorical columns that still exist
cat_cols = [col for col in ['Warehouse_block', 'Mode_of_Shipment', 'Product_importance', 'Gender'] if col in X_train.columns]

label_encoders = {}

# ‚úÖ Step 2: Encode existing categorical columns
for col in cat_cols:
    le = LabelEncoder()
    X_train[col] = le.fit_transform(X_train[col])
    if col in X_test.columns:
        X_test[col] = le.transform(X_test[col])
    label_encoders[col] = le

print(f"‚úÖ Encoded columns: {list(label_encoders.keys())}")

# ‚úÖ Step 3: Get the best trained model (ensure it‚Äôs already fitted)
best_model = models["XGBoost"]

# ‚úÖ Step 4: Save everything for deployment
joblib.dump(best_model, "best_xgboost_model.pkl")
joblib.dump(label_encoders, "label_encoders.pkl")
joblib.dump(X_train.columns.tolist(), "feature_columns.pkl")

print("‚úÖ Model, LabelEncoders, and feature columns saved successfully!")

import joblib

# Load your existing feature columns
feature_columns = joblib.load("feature_columns.pkl")

# Remove ID column if present
if "ID" in feature_columns:
    feature_columns.remove("ID")
    print("‚úÖ 'ID' column removed from feature list.")
else:
    print("‚ÑπÔ∏è 'ID' column not found ‚Äî already clean.")

# Save updated list
joblib.dump(feature_columns, "feature_columns.pkl")
print("üíæ Updated feature_columns.pkl saved successfully.")

import joblib
model = joblib.load("best_xgboost_model.pkl")
print(model.get_booster().feature_names)

import joblib
import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Load your dataset
df = pd.read_csv("/content/Train.csv")  # change path if needed

# Define categorical columns
categorical_cols = ["Warehouse_block", "Mode_of_Shipment", "Product_importance", "Gender"]

# Create dictionary of label encoders
label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le


# Save encoders and feature columns again
joblib.dump(label_encoders, "label_encoders.pkl")
joblib.dump(df.columns.tolist(), "feature_columns.pkl")

print("‚úÖ Encoders and feature columns saved successfully!")
print("‚úÖ Encoded columns:", list(label_encoders.keys()))

import joblib

# Load the trained model
model = joblib.load("best_xgboost_model.pkl")

# Extract and save the feature names directly from the trained model
feature_columns = model.get_booster().feature_names
joblib.dump(feature_columns, "feature_columns.pkl")

print("‚úÖ feature_columns.pkl updated successfully!")
print("Feature columns:", feature_columns)

!pip install streamlit joblib xgboost pandas

# 1. Clean up any running ngrok or Streamlit sessions
!pkill streamlit
!pkill ngrok

!ls -l app.py

!streamlit run app.py &>/content/logs.txt &

!tail -n 10 /content/logs.txt

!pip install pyngrok

from pyngrok import ngrok

# üîë Set your ngrok authtoken properly inside quotes
ngrok.set_auth_token("35F63av7TU78p1He9dq1h2KBm4n_3PndcSNsY69tRmbEJrznW")

# Now create a tunnel to port 8501 (Streamlit)
public_url = ngrok.connect(8501)
print("üåê Streamlit App is live at:", public_url)

# kill any previous streamlit processes
!pkill -f streamlit || true
!pkill -f ngrok || true

# start streamlit in background
!streamlit run app.py &>/content/logs.txt &

# give it a couple seconds to boot
import time
time.sleep(3)

# show last 80 lines of logs so we can see startup errors
print("=== last lines of /content/logs.txt ===")
!tail -n 80 /content/logs.txt

# create/expose tunnel
from pyngrok import ngrok
public_url = ngrok.connect(8501)
print("üåê Public URL:", public_url)
print("‚ö†Ô∏è Keep this Colab tab running to preserve the tunnel.")
